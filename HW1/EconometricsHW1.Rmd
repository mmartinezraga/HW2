---
title: "HW1"
author: "Dorothy Gay, Monica Martinez-Raga, Patrick Sinclair"
date: "9/11/2020"
output: 
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
    toc: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "./")
getwd()
load("acs2017_ny_data.RData")
acs2017_ny[1:10,1:7]
attach(acs2017_ny)
die <- 1:6
dice <- sample(die, size = 20, replace = TRUE)
roll <- function(){
  dice <- sample(die, size = 20, replace = TRUE)
  dice
}
rollw <- function(die = 1:6){
  dice <- sample(die, size = 20, replace = TRUE,
  prob = c(1/10, 1/10, 1/10, 1/10, 1/10, 1/2))
  dice
  }
dicew <- sample(die, size = 20, replace = TRUE,
  prob = c(1/10, 1/10, 1/10, 1/10, 1/10, 1/2))
```
## Question 3 - Dice Roll
### Part 1 - A Fair Die
By setting the interval 1:6 as an R Object **die**, we were able to write a function
that generates a random sample of numbers from the interval. We allowed the sample to replace outcomes so the function simulates a roll of a fair die and set the sample size to 20. 

The command **dice** will produce the same set of randomly generated numbers each time it is executed. If we wish to view different outcomes, we can execute the function **roll**.
```{r fair die 2, echo=TRUE}
dice
roll()
```
Using classical probability, we would expect to see a 6 from the roll of a fair die 1/6 times. For 20 trials, we would expect 6 to be the result 20*(1/6) = 3.33 times. 

In our **dice** trial, we saw a 6 on 4 occasions, or 1/5 times.

### Part 2 - Adjusted Dice

To adjust our die, we created a new function for sampling the object "die" and adjusted the probability within the function. Initially we had changed the sample size and prevented the sample from replacing outcomes but adjusting the probability of each outcome made for a more challenging and interesting learning experience!

```{r, echo=TRUE}
# rollw <- function(die = 1:6){
#   dice <- sample(die, size = 20, replace = TRUE, prob = c(1/10, 1/10, 1/10, 1/10, 1/10, 1/2))
#   dice
#   }
# dicew <- sample(die, size = 20, replace = TRUE, prob = c(1/10, 1/10, 1/10, 1/10, 1/10, 1/2)) 
```

As with our fair die, the object *dicew* will return the same set of weighted outcomes. *rollw* will produce a new set of outcomes each time it is executed.  

```{r}
dicew
rollw()
```


In *dicew*, the probability of rolling a 6 has been adjusted to 1/2, well above the probability of 1/6 for a fair die. We would expect to see a 6 rolled 10 times in 20 trials. The results from *dicew* showed 6 appearing 9 times out of 20, just below what we expected but more than twice as often as we saw in out **dice** trial.

## Question 4 - PUMS Data
### Part 1 - Replicating Commands from R Basics for Lecture I
Below is a summary of all the data.
```{r}
summary(acs2017_ny)
```
The following code shows there are 196,585 rows in the dataset.
```{r}
print(NN_obs <- length(AGE))
```
The mean age of women in NYC is 42.72 years.
```{r}
summary(AGE[female == 1])
```
The mean age of men in NYC is 40.35 years.
```{r}
summary(AGE[!female])
```
Alternative ways to access summary figures:
Female age statistics
```{r}
mean(AGE[female == 1])
sd(AGE[female == 1])
```
Male age statistics
```{r}
mean(AGE[!female])
sd(AGE[!female])
```
### Part 2 - Interesting Finds
For the purpose of exploring interesting statistics in the data, we wanted to find differences in education attainment levels in men and women. The results indicate that women generally attain more and higher levels of formal education, and men had lower rates of high school completion.
```{r}
educ_indx <- factor((educ_nohs + 2*educ_hs + 3*educ_somecoll + 4*educ_college + 5*educ_advdeg), levels=c(1,2,3,4,5),labels = c("No HS","HS","SmColl","Bach","Adv"))
table(educ_indx,female)
```
## Question 5 - SP500 Index
### Part 1 - Mean for time frame February 20, 2020 - September 1, 2020

Here we load in the SP500 data from 2-20-2020 to 9-1-2020
```{r, message=FALSE}
library(readr)
library(dplyr)
library(ggplot2)

S_P <- read_csv("S&P.csv")

```
```{r}
knitr::kable(summary(S_P))
```
Here we will add in a column called "Return", calculated from the day's (Close-Open). Here are the concatenated results. 
```{r}
S_P$Return <- S_P$Close-S_P$Open
knitr::kable(head(S_P, n = 5))
```
From "Return" we can calculate the mean.
```{r}
knitr::kable(mean(S_P$Return))
```
### Part 2 - Mean Return On The Day After a Previous Day's Return Was Positive or Negative
Here we will utilize a Lead/Lag function to determine "Previous Return" based on a previous day's return.
```{r}
S_P$PreviousReturn <- lag(S_P$Return)
knitr::kable(head(S_P, 5))
```

```{r message=FALSE}
ggplot(data=S_P, aes(x=Date, y=Close), name="Close over time") + geom_point() + geom_smooth()
```

Here we filter "PreviousReturn" to find the previous day's return values that are positive. In the summary we can see the mean return on a previous day's positive return is -7.9192.
```{r}
summary(filter(S_P, PreviousReturn >= 0) $Return)
```
Similarly, we can filter by and summarize negative "PreviousReturn" values to find the mean returns on days where the previous day's return values were negative. As shown, the mean returns on these days are 11.06.
```{r}
summary(filter(S_P, PreviousReturn <= 0) $Return)
```
### Part 3 - Mean Return On The Day After a Previous Two Day's Returns Were Positive or Negative
To determine mean on days where the previous two day's returns were negative we simply set another conditional to consider values where returns on both days prior were negative - where the mean is 10.67:
```{r}
S_P$PreviousTwoDayReturn <- lag(S_P$PreviousReturn)
summary(filter(S_P, PreviousReturn <= 0 & PreviousTwoDayReturn <= 0) $Return)
```
And when returns on two days prior are positive where the mean is 2.295:
```{R}
S_P$PreviousTwoDayReturn <- lag(S_P$PreviousReturn)
summary(filter(S_P, PreviousReturn >= 0 & PreviousTwoDayReturn >= 0) $Return)
```
### Part 4 - The Hot Hands Fallacy and How It Relates To Our Findings
```{R}
ReturnsBasedOnHistory <- matrix(c(-7.9192,11.0600,2.2950,10.6700
), ncol=2)
colnames(ReturnsBasedOnHistory) <- c('Past Day Return', 'Past Two Days Return')
rownames(ReturnsBasedOnHistory) <- c('Positive', 'Negative')
ReturnsBasedOnHistory.table <- as.table(ReturnsBasedOnHistory)
ReturnsBasedOnHistory
```
Based on the data from SP500 spanning from February 20, 2020 - September 1, 2020, we notice that the day after a positive return, the mean return is strongly negative. However, the day after two positive returns, the mean return comes up slightly back to positive. Similarly, the day after a negative return, the mean return is starkly positive but after two days the positivity of the return decreases. From here we can infer that with a larger sample size, the returns based on historical movement begin to normalize, approaching the mean of the entire time frame's return of 0.1786.

The time frame we chose was remarkably volatile. During this time, we can define short-term investment as investment based on one day's historical data and longer term investment as based on historical data of greater than one day. According to the Hot Hands Fallacy, short term investors may perceive the market's movement as "hot" if the previous day's returns were either wildly positive or negative, depending on whether they were choosing to buy or sell in relation to the market's movements. According to our data, if they were to act on this perception of a "hot" movement in the short term, they would benefit from the hot hands fallacy so long as they made their moves within the day. However, if they acted on the perception over a period of days, their returns in either direction would have gone down over the amount of time in which they held. Similarly, a long-term investor may be lured by the prospect of making moves and the perception of a "hot" deal, but they were found not to have lost tremendously nor gained tremendously if they were to wait out the high market volatility as the returns approach the slightly positive 9 month mean over time.

### Part 5 - A Question For Discussion
Given institutions have greater ability to be selective in making their predictions when assessing streaks in the market, are they able to leverage trading large volumes to manipulate whether positive return streaks continue or cease?